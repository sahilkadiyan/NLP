{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn5dJpQEwHHp",
        "outputId": "ced4b48a-8d23-407a-9302-c00a179da03a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method 1: Using NLTK**\n",
        "\n",
        "Sentence Tokenization – Splitting sentences in the paragraph for smaller dataset"
      ],
      "metadata": {
        "id": "kBVbLQH3xMNU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjnAkJiFv2JA",
        "outputId": "16ee6e83-e37c-49d7-bdd6-4286f6673fec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello everyone .Welcome to GeeksforGeeks.', 'You are studying NLP article']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Sentence tokenization helps us to split the screen on sentence basis \n",
        "#STEP 1:\n",
        "#Importing library \n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize \n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "#STEP 2:\n",
        "# Creating Text data    \n",
        "text = \"Hello everyone .Welcome to GeeksforGeeks. You are studying NLP article\"\n",
        "\n",
        "#STEP 3:\n",
        "# Implementation of tokenizer\n",
        "sent_tokenize(text) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PunktSentenceTokenizer – When we have huge chunks of data then it is efficient to use it."
      ],
      "metadata": {
        "id": "qqdcFBRXxltk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 1:\n",
        "#Importing library \n",
        "import nltk.data \n",
        "\n",
        "#STEP 2:\n",
        "# Creating Text data  \n",
        "# Loading PunktSentenceTokenizer using English pickle file \n",
        "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle') \n",
        "  \n",
        "#STEP 3:\n",
        "# Implementation of tokenizer\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2Ih09tfxnUG",
        "outputId": "1111011e-50db-42f5-9e5a-991e8056ffd8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello everyone .Welcome to GeeksforGeeks.', 'You are studying NLP article']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize sentence of different language – One can also tokenize sentence from different languages using different pickle file other than English."
      ],
      "metadata": {
        "id": "kMqDtEXvyoL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 1:\n",
        "#Importing library \n",
        "import nltk.data \n",
        "\n",
        "#STEP 2:\n",
        "# Creating Text data    \n",
        "spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle') \n",
        "\n",
        "#STEP 3:\n",
        "# Implementation of tokenizer\n",
        "text = 'Hola amigo. Estoy bien.'\n",
        "spanish_tokenizer.tokenize(text) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mp0V9lFyre5",
        "outputId": "3540656b-a9f3-4fd4-f204-ea4c8eb80444"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hola amigo.', 'Estoy bien.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 1:\n",
        "#Importing library\n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "#STEP 2:\n",
        "# Creating Text data   \n",
        "text = \"Hello everyone. Welcome to GeeksforGeeks.\"\n",
        "\n",
        "#STEP 3:\n",
        "# Implementation of tokenizer\n",
        "word_tokenize(text) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v35JKYTTmfOg",
        "outputId": "be235b67-c914-4f21-e46d-bd09c944d043"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'everyone', '.', 'Welcome', 'to', 'GeeksforGeeks', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE : These tokenizers work by separating the words using punctuation and spaces.\n",
        "# And as mentioned in the code outputs below it does not discard the punctuation, \n",
        "# allowing a user to decide what to do with the punctuations at the time of pre-processing.\n",
        "\n",
        "#STEP 1:\n",
        "#Importing library\n",
        "from nltk.tokenize import TreebankWordTokenizer \n",
        "\n",
        "#STEP 2:\n",
        "# Creating Text data   \n",
        "tokenizer = TreebankWordTokenizer() \n",
        "\n",
        "#STEP 3:\n",
        "# Implementation of tokenizer\n",
        "tokenizer.tokenize(text) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss9DHRokmyF8",
        "outputId": "b92fb936-e994-4cb0-bb62-4e3e759e1f01"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'everyone.', 'Welcome', 'to', 'GeeksforGeeks', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PunktWordTokenizer – It doen’t seperates the punctuation from the words.\n",
        "\n",
        "#STEP 1:\n",
        "#Importing library\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "WordPunctTokenizer().tokenize(\"Let's see how it's working.\")\n",
        "\n",
        "# #STEP 2:\n",
        "# # Creating Text data     \n",
        "# tokenizer = PunktWordTokenizer() \n",
        "\n",
        "# #STEP 3:\n",
        "# # Implementation of  tokenizer\n",
        "# tokenizer.tokenize(\"Let's see how it's working.\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNwb214km4a8",
        "outputId": "e634de19-ac93-4fc8-ce72-a240fec906a4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Let', \"'\", 's', 'see', 'how', 'it', \"'\", 's', 'working', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        " #WordPunctTokenizer – It seperates the punctuation from the words.\n",
        "\n",
        "from nltk.tokenize import WordPunctTokenizer \n",
        "tokenizer = WordPunctTokenizer() \n",
        "tokenizer.tokenize(\"Let's see how it's working.\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2qDfMsInZJu",
        "outputId": "3985c3e5-ec5d-4569-ca44-f83e2170978f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Let', \"'\", 's', 'see', 'how', 'it', \"'\", 's', 'working', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# most imp\n",
        "#Code 7: Using Regular Expression\n",
        "\n",
        "#STEP 1:\n",
        "#Importing library\n",
        "from nltk.tokenize import RegexpTokenizer \n",
        "\n",
        "#STEP 2:\n",
        "# Creating Text data  \n",
        "tokenizer = RegexpTokenizer(\"[\\w']+\") \n",
        "#\\w+ matches any word character and here \\w' is for let's and it's word\n",
        "text = \"Let's see how it's working.\"\n",
        "\n",
        "#STEP 3:\n",
        "# Implementation of tokenizer\n",
        "tokenizer.tokenize(text) \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDG51XqcnhMQ",
        "outputId": "80fc6788-a66e-471e-e6ef-c05952a38f4c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Let's\", 'see', 'how', \"it's\", 'working']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}